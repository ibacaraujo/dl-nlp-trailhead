{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workbook_Word_Meaning_and_Word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibacaraujo/dl-nlp-trailhead/blob/master/Workbook_Word_Meaning_and_Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lyLzwlGH2jQL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Meaning and Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "SkEjc9HI2112",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook provides a general introduction to natural language processing (NLP) using Word2vec. If you haven't finished the first module, [Deep Learning and Natural Language Processing](https://trailhead.salesforce.com/content/learn/modules/deep-learning-and-natural-language-processing), we highly suggest starting there.\n",
        "\n",
        "Code sections of the notebook appear in grey cells. To run the code in a cell, hover over the brackets in the upper left corner of the cell and click the play button or **Shift+Enter**. You can edit the code in any cell. When running a cell, be sure that you've run all the above cells first to avoid errors.\n",
        "\n",
        "When you have completed the lab, return to Trailhead to enter your answers to the exercises in the quiz section and get points."
      ]
    },
    {
      "metadata": {
        "id": "F2K8ruAnolV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a8081bce-a6c9-4893-e15c-2729373236ac"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import os\n",
        "import urllib\n",
        "import zipfile\n",
        "import collections\n",
        "import math\n",
        "import os\n",
        "import datetime as dt\n",
        "import string\n",
        "import re\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "def println(*x):\n",
        "  print(*x)\n",
        "  print()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.0 from http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 32.1MB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5c4a2000 @  0x7f356ff982a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eHkArH5tQpqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Extract data"
      ]
    },
    {
      "metadata": {
        "id": "hHs4TXl2TSBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Description\n",
        "We'll be playing around with two datasets to get a sense of how the underlying data affects the training of the word vectors. The first dataset is a large chunk of text that comes from English Wikipedia, and the second is a set of movie reviews from the Stanford Sentiment Treebank (SST)."
      ]
    },
    {
      "metadata": {
        "id": "2BZNFWQSWR3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the data"
      ]
    },
    {
      "metadata": {
        "id": "n-zCk81JRQsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "07cb5563-dd49-4658-b44b-d8eeedb2d998"
      },
      "cell_type": "code",
      "source": [
        "en_wiki_url='http://mattmahoney.net/dc/text8.zip'\n",
        "sst_url = 'https://raw.githubusercontent.com/salesforce/decaNLP/master/local_data/train_fine_sent.csv'\n",
        "\n",
        "def download(url):\n",
        "    filename = os.path.basename(url)\n",
        "    if not os.path.exists(filename):\n",
        "        downloaded_path, _ = urllib.request.urlretrieve(url, filename)\n",
        "    else:\n",
        "      downloaded_path = filename\n",
        "    return downloaded_path\n",
        "\n",
        "en_wiki_path = download(en_wiki_url)\n",
        "sst_path = download(sst_url)\n",
        "\n",
        "print('English Wikipedia (small): ', en_wiki_path)\n",
        "print('Stanford Sentiment Treebank: ', sst_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Wikipedia (small):  text8.zip\n",
            "Stanford Sentiment Treebank:  train_fine_sent.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LAgVBn9PWbHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract sentences"
      ]
    },
    {
      "metadata": {
        "id": "DKQxsPECSEEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_wiki_sentences, sst_sentences = None, None\n",
        "\n",
        "# The English Wikipedia data has already been cleaned, tokenized, and turned \n",
        "# into a long string of words that can be considered a single sentence\n",
        "with zipfile.ZipFile(en_wiki_path) as f:\n",
        "  words = f.read(f.namelist()[0]).split()\n",
        "  en_wiki_sentences = [[w.decode() for w in words]]\n",
        "\n",
        "# We need to clean the data from the movie reviews\n",
        "def clean(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "  \n",
        "  \n",
        "with open(sst_path) as f:\n",
        "  # skip the labels of the csv file\n",
        "  next(f) \n",
        "  # skip the label and the comma, then clean\n",
        "  sst_sentences = [clean(s[2:]).split() for s in f] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "461he40Z0QiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "06ab77d1-b208-472c-c7e0-01c580621038"
      },
      "cell_type": "code",
      "source": [
        "print('English Wikipedia (small) example: ', en_wiki_sentences[0][:10])\n",
        "print('Stanford Sentiment Treebank example: ', sst_sentences[0][:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Wikipedia (small) example:  ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
            "Stanford Sentiment Treebank example:  ['stirring', 'funny', 'and', 'finally', 'transporting', 'reimagining', 'of', 'beauty', 'and', 'beast']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_01Se0Zo0P7t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 1:\n",
        "What is the first word in the first example of the English Wikipedia dataset?"
      ]
    },
    {
      "metadata": {
        "id": "zNgUB0xb0asD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 2:\n",
        "What is the first word in the first example of the SST dataset?"
      ]
    },
    {
      "metadata": {
        "id": "Mi7mSUutWe2x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create a vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "5Juz_MJ_gvu5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we've downloaded and extracted our data, we need to construct a vocabulary. This code creates two vocabularies for each dataset—one that contains all the words that appear in the sentences, and one that contains only words that appear five or more times. Then we'll take a look at both the most common words in each vocabulary, as well as what our earlier example sentences look like with uncommon words removed."
      ]
    },
    {
      "metadata": {
        "id": "RMmMc52GW1Or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "385ef053-7a19-41db-9080-2fbc03bfed42"
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  \n",
        "  def __init__(self, sentences):\n",
        "    word_counts = collections.Counter([w for s in sentences for w in s ])\n",
        "    print('Sentences contain ', len(word_counts), ' words.')\n",
        "    common_word_counts = {word: count \n",
        "                          for word, count in word_counts.items() if count >= 5}\n",
        "    print('Sentences contain ', len(common_word_counts), ' words that occur at least 5 times.')\n",
        "                                 \n",
        "    # Replace uncommon words with a special unknown token\n",
        "    unk_token = 'UNK'\n",
        "    unk_token_count = 0\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "      new_sentence = []\n",
        "      for word in sentence:\n",
        "        if word in common_word_counts:\n",
        "          new_sentence.append(word)\n",
        "        else:\n",
        "          unk_token_count += 1\n",
        "          new_sentence.append(unk_token)\n",
        "      new_sentences.append(new_sentence)\n",
        "    self.sentences = new_sentences\n",
        "                            \n",
        "    if unk_token in common_word_counts:\n",
        "      common_word_counts[unk_token] += unk_token_count\n",
        "    else:\n",
        "      common_word_counts[unk_token] = unk_token_count\n",
        "    num_tokens = sum(common_word_counts.values())\n",
        "                                     \n",
        "    self.index_to_word, self.word_to_index, self.word_to_frequency = {}, {}, {}\n",
        "    sorted_common_word_counts = sorted(common_word_counts.items(), \n",
        "                                       key=lambda tup: (-tup[1], tup[0]))\n",
        "\n",
        "    for idx, (word, count) in enumerate(sorted_common_word_counts):\n",
        "      self.index_to_word[idx] = word\n",
        "      self.word_to_index[word] = idx \n",
        "      self.word_to_frequency[word] = count / num_tokens\n",
        "\n",
        "print('Wikipedia (full | uncommon words removed)')\n",
        "en_wiki_vocab = Vocabulary(en_wiki_sentences)\n",
        "print('\\nSST (full | uncommon words removed)')\n",
        "sst_vocab = Vocabulary(sst_sentences)\n",
        "\n",
        "en_wiki_unked = en_wiki_vocab.sentences\n",
        "sst_unked = sst_vocab.sentences"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wikipedia (full | uncommon words removed)\n",
            "Sentences contain  253854  words.\n",
            "Sentences contain  71290  words that occur at least 5 times.\n",
            "\n",
            "SST (full | uncommon words removed)\n",
            "Sentences contain  17008  words.\n",
            "Sentences contain  3385  words that occur at least 5 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r_uBi2eR0rJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f5b9f444-4e11-475d-c8a2-50713e76bdfe"
      },
      "cell_type": "code",
      "source": [
        "print('Wikipedia Vocabulary')\n",
        "print(len(en_wiki_vocab.index_to_word))\n",
        "print(list(en_wiki_vocab.index_to_word[i] for i in range(10)))\n",
        "print(en_wiki_unked[0][:10])\n",
        "\n",
        "print('\\nSST Vocabulary')\n",
        "print(len(sst_vocab.index_to_word))\n",
        "print(list(sst_vocab.index_to_word[i] for i in range(10)))\n",
        "print(sst_unked[0][:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wikipedia Vocabulary\n",
            "71291\n",
            "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'UNK', 'zero', 'nine']\n",
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
            "\n",
            "SST Vocabulary\n",
            "3386\n",
            "['UNK', 'and', 'of', 'to', 'is', 'in', 'that', 'its', 'it', 'as']\n",
            "['stirring', 'funny', 'and', 'finally', 'UNK', 'UNK', 'of', 'beauty', 'and', 'beast']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5RWUpbu0qr3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 3:\n",
        "What is the most common word in the English Wikipedia vocabulary constructed above? "
      ]
    },
    {
      "metadata": {
        "id": "Pj2BRyhv0078",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 4:\n",
        "Which words were replaced by the UNK token in the first SST sentence?"
      ]
    },
    {
      "metadata": {
        "id": "HcthE-YKeuN_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Drop words based on frequency\n",
        "\n",
        "The most frequent words usually provide less information. For\n",
        "example, nearly every word co-occurs frequently with “the”. \n",
        "For this reason, we discard words with a probability related to its frequency and a threshold typically set to 10e-5."
      ]
    },
    {
      "metadata": {
        "id": "gTZdQEaAexan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def drop_words(sentences, vocabulary, threshold=10e-5):\n",
        "  new_sentences = []\n",
        "  for sentence in sentences:\n",
        "    new_sentence = []\n",
        "    for word in sentence:\n",
        "      word_frequency = vocabulary.word_to_frequency[word]\n",
        "      discard_probability = 1 - np.sqrt(threshold / word_frequency)\n",
        "      if random.random() > discard_probability:\n",
        "        new_sentence.append(word)\n",
        "    if len(new_sentence) > 0:\n",
        "        new_sentences.append(new_sentence)\n",
        "  return new_sentences\n",
        "\n",
        "random.seed(123)\n",
        "\n",
        "en_wiki_dropped = drop_words(en_wiki_unked, en_wiki_vocab)\n",
        "sst_dropped = drop_words(sst_unked, sst_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "In8nLIpm2tEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4894fa8a-bb67-42b7-b548-7ad1e4b77e82"
      },
      "cell_type": "code",
      "source": [
        "print('Wikipedia vocabulary with dropped words')\n",
        "print(en_wiki_dropped[0][:10])\n",
        "\n",
        "print('\\nSST vocabulary with dropped words')\n",
        "print(sst_dropped[0][:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wikipedia vocabulary with dropped words\n",
            "['anarchism', 'originated', 'term', 'abuse', 'used', 'working', 'radicals', 'diggers', 'revolution', 'sans']\n",
            "\n",
            "SST vocabulary with dropped words\n",
            "['stirring', 'finally', 'beauty', 'beast']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9YROkeYM1X1v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 5:\n",
        "What was the first word dropped from the English WIkipedia example?"
      ]
    },
    {
      "metadata": {
        "id": "jc8WQNjf1efT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 6:\n",
        "What was the first word dropped from the SST example?"
      ]
    },
    {
      "metadata": {
        "id": "AjYYJ7ZuqNWv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hands-on: Numericalize the data"
      ]
    },
    {
      "metadata": {
        "id": "JO2o8Bpwn_A8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's your turn to write some code! Implement the ```numericalize``` method. This method takes in a sentence and a vocabulary and returns a list containing the indices of each word in the sentence, in the same order, according to the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "UZGqiYplqPYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def numericalize(sentence, vocabulary):\n",
        "  # TODO: Implement\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVvMkDR2OEo5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_wiki_numericalized = [numericalize(s, en_wiki_vocab) for s in en_wiki_dropped]\n",
        "sst_numericalized = [numericalize(s, sst_vocab) for s in sst_dropped]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tt74HJIfOF6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(en_wiki_numericalized[0][:10])\n",
        "print(sst_numericalized[0][:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKjYtbgp2KZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 7:\n",
        "What is the first index in the English Wikipedia example after numericalization?"
      ]
    },
    {
      "metadata": {
        "id": "TIQLtXTE2gbq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 8:\n",
        "What is the last index in the SST example after numericalization?"
      ]
    },
    {
      "metadata": {
        "id": "MqTi4hozv8Ed",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Variations\n",
        "There are four variations of training Word2Vec.\n",
        "\n",
        "Your Word2Vec model can either be skip-gram (nSG) or continuous bag-of-words (nCBOW) where n represents the wondow size.\n",
        "It can also be trained with a full softmax (FS) or with k-negative sampling (kNS). The difference between these variants really comes down to how training examples are constructed.\n",
        "\n",
        "In order to construct a training example, we:\n",
        "\n",
        "\n",
        "1. Choose a random sentence from the dataset.\n",
        "2. Choose a random word in the sentence  and consider the context window to be the word along with up to n-1 words to its right. If the context window has length less than or equal to n//2, return to Step 1.\n",
        "3. Let center_word be the word at the middle of the context window.\n",
        "4. Let context_words be the set of all other words in the context window.\n",
        "5. Let context_word be one of the words in context_words chosen at random.\n",
        "5. If using the nSG model, let example be [center_word, context_word].\n",
        "6. If using the nCBOW model, let example be [context_words, center_word].\n",
        "7. If using a kNS model, extend example to be a triple with the final entry a list of k randomly sampled words from the vocabulary.\n",
        "\n",
        "\n",
        "* The nSG models predicts context words from a center word and thus capture the idea that any given word should be predictive of nearby words.\n",
        "\n",
        "* The nCBOW models predict the center word from nearby context words and thus capture the idea that a word should be predictable based on its context.\n",
        "\n",
        "*   The kNS models are a trick to save memory over the FS models. Rather than computing a probability for every single word in the vocabulary every time we want to predict a target word (center word for CBOW and context word for SG, respectively), we just predict whether the target word is likely and whether k randomly sampled words are likely. This mimics the behavior of a full softmax prediction, which trains the model to up the probability of the target word and lower the probability of all other words. In the case of kNS, we up the probability of the target word and lower the probability of k other words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1CobyyA3wUdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hands-on: Construct examples for each W2V variant"
      ]
    },
    {
      "metadata": {
        "id": "mfWTBktrqi8B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this exercise, you'll implement parts of the ```construct_examples``` method. First, grab a random sentence from your numericalized vocabulary and construct your context window of size n from a random word in that sentence. Next, pick your center and context words. Finally, create examples for the skip-gram and CBOW models."
      ]
    },
    {
      "metadata": {
        "id": "9vFn-cjB3GN1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def construct_examples(numericalized_sentences, vocabulary, num_examples=int(1e6), n=5, sg=True, k=0):\n",
        "  examples = []\n",
        "  while True:\n",
        "    # TODO: select a random sentence index using random.randint and get that\n",
        "    # sentence. Be careful to avoid indexing errors.\n",
        "    sentence_idx = \n",
        "    sentence = \n",
        "    # TODO: Select a random window index using random.randint\n",
        "    # and obtain that window of size n. Be careful to avoid indexing errors.\n",
        "    window_idx = \n",
        "    window = \n",
        "    \n",
        "    if len(window) <= n//2:\n",
        "      continue\n",
        "      \n",
        "    # TODO: Get the center word and the context words \n",
        "    center_word = \n",
        "    context_words = \n",
        "    \n",
        "    # TODO: Create examples using the guidelines above\n",
        "    if sg: # if Skip-Gram\n",
        "      context_word = context_words[random.randint(0, len(context_words)-1)]\n",
        "      example = \n",
        "    else: # if CBOW\n",
        "      example = \n",
        "      if len(window) < n:\n",
        "        continue\n",
        "      \n",
        "    if k > 0: # if doing negative sampling\n",
        "      samples = [random.randint(0, len(vocabulary.index_to_word)-1) \n",
        "                 for _ in range(k)]\n",
        "      example.append(samples)\n",
        "      \n",
        "    examples.append(example)\n",
        "    if len(examples) >= num_examples:\n",
        "      break\n",
        "  \n",
        "  return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YX8Zpv4PeOnu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "\n",
        "print('Constructing English Wikipedia examples for 5SG-FS model')\n",
        "en_wiki_5sgfs_examples = construct_examples(en_wiki_numericalized, en_wiki_vocab)\n",
        "print('Constructing English Wikipedia examples for 5CBOW-FS model')\n",
        "en_wiki_5cbowfs_examples = construct_examples(en_wiki_numericalized, en_wiki_vocab, sg=False)\n",
        "print('Constructing English Wikipedia examples for 5SG-15NS model')\n",
        "en_wiki_5sg15ns_examples = construct_examples(en_wiki_numericalized, en_wiki_vocab, k=15)\n",
        "print('Constructing English Wikipedia examples for 5CBOW-15NS model')\n",
        "en_wiki_5cbow15ns_examples = construct_examples(en_wiki_numericalized, en_wiki_vocab, sg=False, k=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBdhJrw55PPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Constructing Stanford Sentiment Treebank examples for 5SG-15NS model')\n",
        "sst_5sgfs_examples = construct_examples(sst_numericalized, sst_vocab)\n",
        "print('Constructing Stanford Sentiment Treebank examples for 5CBOW-15NS model')\n",
        "sst_5cbowfs_examples = construct_examples(sst_numericalized, sst_vocab, sg=False)\n",
        "print('Constructing Stanford Sentiment Treebank examples for 5SG-15NS model')\n",
        "sst_5sg15ns_examples = construct_examples(sst_numericalized, sst_vocab, k=15)\n",
        "print('Constructing Stanford Sentiment Treebank examples for 5CBOW-15NS model')\n",
        "sst_5cbow15ns_examples = construct_examples(sst_numericalized, sst_vocab, sg=False, k=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZIxZEwicFmRp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construct minibatches for the FS models\n",
        "A minibatch consists of multiple examples grouped together as torch tensors. We want to batch the inputs and the targets so the model can process them in parallel.\n",
        "\n",
        "If we're using a full softmax, minibatching is very simple. The inputs are whatever the first entry of an example is and the targets are whatever the second entry in the example is. This should be as expected."
      ]
    },
    {
      "metadata": {
        "id": "mUKAw5BRGXuf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tensorize(x, dtype=torch.long):\n",
        "  if len(x) == 0:\n",
        "    return None\n",
        "  return torch.tensor(x, dtype=dtype)\n",
        "\n",
        "def batchFS(examples, batch_size=64):\n",
        "  example_indices = random.sample(range(0, len(examples)-1), batch_size)\n",
        "  batch_examples = [examples[idx] for idx in example_indices]\n",
        "  inputs = [example[0] for example in batch_examples]\n",
        "  targets = [example[1] for example in batch_examples]\n",
        "  return [tensorize(inputs), tensorize(targets)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uf4aoig9NyMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "for x in batchFS(sst_5sgfs_examples):\n",
        "  print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ftPBUhhBgpOL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 9:\n",
        "What is the last index in the last tensor printed out above for the batched examples?"
      ]
    },
    {
      "metadata": {
        "id": "5_EFbd-zLvUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construct minibatches for the kNS models\n",
        "Negative sampling requires three tensors instead of two. The first tensor represents the inputs, just like before. The second is an output tensor with each row containing first the target word index and then the negative sample word indices. The third is a labels tensor representing each of the entries in the output tensor as a 1 if it is the target word and a 0 if it is a negative sample. "
      ]
    },
    {
      "metadata": {
        "id": "the9_niQL3U6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batchkNS(examples, batch_size=64):\n",
        "  example_indices = random.sample(range(0, len(examples)-1), batch_size)\n",
        "  batch_examples = [examples[idx] for idx in example_indices]\n",
        "  inputs = [example[0] for example in batch_examples]\n",
        "  targets = [example[1] for example in batch_examples]\n",
        "  negatives = [example[2] for example in batch_examples]\n",
        "  outputs = torch.cat([tensorize(targets).unsqueeze(1), tensorize(negatives)], dim=1)\n",
        "  labels = torch.zeros_like(outputs, dtype=torch.float)\n",
        "  labels[:, 0] = 1.\n",
        "  \n",
        "  return [tensorize(inputs), outputs, labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nupWXudyIP81",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "for x in batchkNS(en_wiki_5cbow15ns_examples):\n",
        "  print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pY9Hj3E4gzTR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 10:\n",
        "What is the first index in the first tensor printed out above for the batched examples?"
      ]
    },
    {
      "metadata": {
        "id": "RWI2-y2VVCJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construct a generic batch function\n",
        "We can now wrap our two batch construction functions in a way that's generic and adapts to the kind of examples we pass in. Later on, this flexibility will be very convenient. You can verify that as long as the random seed is the same, the batches are in fact identical to those constructed earlier."
      ]
    },
    {
      "metadata": {
        "id": "cR4XjZb2VFYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch(examples, batch_size=64):\n",
        "  assert len(examples) > 0\n",
        "  \n",
        "  if len(examples[0]) == 2:\n",
        "    return batchFS(examples, batch_size=batch_size)\n",
        "  else:\n",
        "    return batchkNS(examples, batch_size=batch_size)\n",
        " \n",
        "random.seed(123)\n",
        "for x in batch(sst_5sgfs_examples):\n",
        "  print(x)\n",
        "print()\n",
        "random.seed(123) \n",
        "for x in batch(en_wiki_5cbow15ns_examples):\n",
        "  print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KWDN2a0fOdXM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hands-on: Define the Word2Vec models\n",
        "We'll define a single model that handles all variants of Word2Vec because there is so much overlap in how the models work. A Word2VecModel has a set of vectors that are trainable parameters. These are of some size embedding_size. The first half of these vectors is treated as input embeddings for each word, and the second half is treated as output embeddings (either for the softmax or the partial softmax used in negative sampling). These vectors are randomly initialized, but trained using one of the Word2Vec variants. Ultimately, the models return a tensor of scores with size of either the entire vocabulary (in the case of FS, scores will have size batch_size x vocabulary_size) or over the target words and their respective negative samples (in this case, scores will have size batch_size x (k +1)).\n",
        "\n",
        "The inline comments guide you through this process in detail."
      ]
    },
    {
      "metadata": {
        "id": "tkMwhZxlOgFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Word2VecModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, embedding_size=300):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    # TODO: Use randn to initialize a tensor for vocab_size vectors each\n",
        "    # of size embedding_size\n",
        "    vectors = \n",
        "    self.vectors = nn.Parameter(vectors)\n",
        "    \n",
        "  def forward(self, batch, samples=None):\n",
        "    inputs = batch[0]\n",
        "    # TODO: Obtain the input vector portion (the first self.embedding_size//2\n",
        "    # entries of the input vectors) of self.vectors for inputs\n",
        "    input_vectors = \n",
        "    \n",
        "    # TODO: If this is a CBOW model, \n",
        "    # compute a continous bag-of-words over the input vectors\n",
        "    if input_vectors.dim() == 3:\n",
        "      input_vectors = \n",
        "\n",
        "    if len(batch) == 2: # Full Softmax\n",
        "      # TODO: obtain the output portion of all vectors \n",
        "      # HINT: you can index into the last dimension of tensors with any number \n",
        "      # of dimensions by using the following notation tensor[..., idx]\n",
        "      target_vectors = \n",
        "      # TODO: compute scores between input and output vectors\n",
        "      # via matrix multiplication (you'll need to transpose output_vectors)\n",
        "      scores = \n",
        "    else: # Negative Sampling\n",
        "      outputs = batch[1]\n",
        "      # TODO: obtain the output vectors only for samples\n",
        "      output_vectors = \n",
        "      # TODO: compute scores between the input vectors and the output vectors\n",
        "      # First, you'll need to expand the input vectors along dimension 1 using \n",
        "      # unsqueeze() so that the input vectors are now of shape (batch_size, 1, embedding_size)\n",
        "      input_vectors = \n",
        "      # Then you'll need to transpose the output_vectors along dimensions 1 and 2\n",
        "      # so that the output_vectors is of shape (batch_size, embedding_size, k + 1)\n",
        "      output_vectors = \n",
        "      # Now a matrix multiply should yield a tensor of size (batch_size, 1, k + 1)\n",
        "      # and you can get the matrix of scores by calling squeeze() to get a tensor\n",
        "      # of size (batch_size, k + 1), which should match the size of labels\n",
        "      scores = \n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWE1B9BCUY9W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Word2Vec\n",
        "Now we need to define a training loop that trains our models for some number of iterations. The inline comments walk you through this process."
      ]
    },
    {
      "metadata": {
        "id": "SGQhjZoMUhmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_trainable_parameters(model):\n",
        "  \"\"\"Returns the trainable parameters of a model\"\"\"\n",
        "  return list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "\n",
        "def denumericalize(vocab, x):\n",
        "  return [vocab.index_to_word[y] for y in x]\n",
        "\n",
        "\n",
        "def train(model, vocab, dataset, device, max_iterations=int(1e6), log_every=1e4,\n",
        "          val_every=1e5, num_val_samples=50, val_head=500, num_neighbors=5,\n",
        "          anneal_every=int(1e8), batch_size=256):\n",
        "  \"\"\"Trains a Word2VecModel on a Word2VecDataset for max_iterations (int)\"\"\"\n",
        "  print('Training on ', len(dataset), ' examples for '\n",
        "        , max_iterations, ' iterations with ', len(vocab.index_to_word), ' words in the vocabulary.')\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  # TODO: initialize a default Adam optimizer\n",
        "  # hint: use get_trainable_parameters\n",
        "  opt =  \n",
        "  avg_loss = 0\n",
        "  for iteration in range(max_iterations):\n",
        "    # TODO: zero out the gradients the optimizer is tracking\n",
        "    \n",
        "    # TODO: get the next batch from the dataset\n",
        "    b = \n",
        "    b = [x.to(device) for x in b]\n",
        "    # TODO: get scores from the model\n",
        "    scores = \n",
        "\n",
        "    if len(b) == 2: # Full Softmax\n",
        "      targets = b[1]\n",
        "      # TODO: use scores and targets to compute a cross entropy loss\n",
        "      loss = \n",
        "    else:\n",
        "      labels = b[2]\n",
        "      # TODO: use the scores and labels to compute the loss using\n",
        "      # a binary cross entropy with logits loss function\n",
        "      loss = \n",
        "    \n",
        "    # TODO: compute gradients using the loss\n",
        "\n",
        "    # TODO: update your parameters by using the optimizer to take a step\n",
        "    \n",
        "    \n",
        "    # Annealing the learning rate\n",
        "    if (iteration + 1) % anneal_every == 0:\n",
        "      print('Annealing')\n",
        "      opt.param_groups[0]['lr'] *= 0.5\n",
        "      \n",
        "    # logging  \n",
        "    avg_loss += loss.item()\n",
        "    if (iteration + 1) % log_every == 0:\n",
        "      print(f'Iteration: {iteration + 1}, avg_loss: {avg_loss / log_every}')\n",
        "      avg_loss = 0\n",
        "      \n",
        "    # validating by checking nearest neighbors\n",
        "    if (iteration + 1) % val_every == 0:\n",
        "      model.eval()\n",
        "      print('\\nValidating:\\n')\n",
        "      normalized_vectors = F.normalize(model.vectors)\n",
        "      random_samples = random.sample(range(int(val_head)), num_val_samples)\n",
        "      valid_scores = torch.matmul(normalized_vectors[random_samples], normalized_vectors.transpose(0, 1))\n",
        "      nearest_neighbors = valid_scores.detach().cpu().numpy().argsort(axis=1)[:, -(num_neighbors+1):-1]\n",
        "      for i, row in enumerate(nearest_neighbors):\n",
        "        key_word = denumericalize(vocab, [random_samples[i]])\n",
        "        neighbors = denumericalize(vocab, row.tolist())\n",
        "        neighbors.reverse()\n",
        "        print(f'word: {key_word} --', f'neighbors: {neighbors}') \n",
        "      print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7E1B8SSZRJe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Skip-Gram models\n",
        "\n",
        "Below are some examples of how to use the code we've written to train word vectors. Often it takes much more data and a large number of iterations in order to get high quality word vectors, but you can see some reasonable patterns developing using the examples below. Notice that the word vectors from English Wikipedia might give more intuitive neighbors than those from SST because Wikipedia is a much larger dataset and less domain specific."
      ]
    },
    {
      "metadata": {
        "id": "sV_0UO6ix-zu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Note: Enable GPU\n",
        "Training models takes a long time. You can speed it up by going to **Edit > Notebook Settings** and changing the hardware accelerator to **GPU**. Leave this setting for the remainder of the exercise to speed up the code runs."
      ]
    },
    {
      "metadata": {
        "id": "cgyzkRbEZl-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Full softmax"
      ]
    },
    {
      "metadata": {
        "id": "ZI3bVfPj1NkV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the code below to train word vectors with full softmax for our SST dataset."
      ]
    },
    {
      "metadata": {
        "id": "arxNPhdcZqZQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Stanford Sentiment Treebank"
      ]
    },
    {
      "metadata": {
        "id": "M2bN5LzjEHyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(123)\n",
        "random.seed(123)\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "vocab_size = len(sst_vocab.index_to_word)\n",
        "start_time = time.time()\n",
        "train(Word2VecModel(vocab_size=vocab_size), sst_vocab, sst_5sgfs_examples, device)\n",
        "print('Time Elapsed: ', time.time() - start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsltpVZawEqg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 11:\n",
        "What is the the fifth nearest neighbor (the one all the way to the right) for 'suspense'?"
      ]
    },
    {
      "metadata": {
        "id": "EnirHEM_wKFv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 12:\n",
        "What is the the  nearest neighbor (the one all the way to the left) for 'death'?"
      ]
    },
    {
      "metadata": {
        "id": "rdGr77mhiCTR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### English Wikipedia"
      ]
    },
    {
      "metadata": {
        "id": "zCvjqE6miD-5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(123)\n",
        "random.seed(123)\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "vocab_size = len(en_wiki_vocab.index_to_word)\n",
        "start_time = time.time()\n",
        "train(Word2VecModel(vocab_size=vocab_size), en_wiki_vocab, en_wiki_5sgfs_examples, device, log_every=1000, val_every=10000, max_iterations=100000)\n",
        "print('Time Elapsed: ', time.time() - start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qqhkZGwwOkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz Question 13:\n",
        "What are the nearest neighbors for 'minister'?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xiWDVXGPzKV9"
      },
      "cell_type": "markdown",
      "source": [
        "# Train CBOW models\n",
        "\n",
        "There won't be quiz questions on these models as they're a bit less reliable than the full softmax, skip-gram models, but we encourage you to play around with them!"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xzj4XcUPzKV-"
      },
      "cell_type": "markdown",
      "source": [
        "### Full Softmax"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Kn9kPsJgzKV-"
      },
      "cell_type": "markdown",
      "source": [
        "#### Stanford Sentiment Treebank"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6CsNVIKezKWA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(123)\n",
        "random.seed(123)\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "vocab_size = len(sst_vocab.index_to_word)\n",
        "start_time = time.time()\n",
        "train(Word2VecModel(vocab_size=vocab_size), sst_vocab, sst_5cbowfs_examples, device)\n",
        "print('Time Elapsed: ', time.time() - start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eCAgu5YizKWB"
      },
      "cell_type": "markdown",
      "source": [
        "### Negative Sampling"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yo6yI9m2zKWC"
      },
      "cell_type": "markdown",
      "source": [
        "#### English Wikipedia"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GWvJn7T3zKWC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(123)\n",
        "random.seed(123)\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "vocab_size = len(en_wiki_vocab.index_to_word)\n",
        "start_time = time.time()\n",
        "train(Word2VecModel(vocab_size=vocab_size), en_wiki_vocab, en_wiki_5cbow15ns_examples, device, log_every=1000,  val_every=10000, max_iterations=100000)\n",
        "print('Time Elapsed: ', time.time() - start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}